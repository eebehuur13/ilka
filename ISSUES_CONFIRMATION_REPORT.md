# ILKA/ZYN Issues Confirmation Report

**Date:** October 17, 2025  
**Analyzed by:** Factory Droid  
**Request:** Verify issues identified by "smart person"

---

## EXECUTIVE SUMMARY

**‚úÖ ALL MAJOR ISSUES CONFIRMED**

Every issue mentioned by the smart person exists in the ilka/zyn codebase. The analysis is accurate and identifies critical architectural problems that explain why zyn struggles compared to RAG.

---

## ISSUE 1: BM25 SCORING INSTABILITY ‚úÖ CONFIRMED

### The Problem

**Location:** `ilka/src/retrieval/bm25.ts:57` and `ilka/migrations/0001_initial_schema.sql:45`

**Confirmed Code:**

```typescript
// ilka/src/retrieval/bm25.ts:57
// Get GLOBAL corpus passage count for correct IDF calculation
const globalStats = await this.db
  .prepare('SELECT COUNT(*) as total FROM passages')
  .first();
const totalPassages = (globalStats?.total as number) || passages.length;

// Line 94-98: IDF calculation uses GLOBAL totalPassages
for (const [term, docSet] of documentFrequency.entries()) {
  const df = docSet.size;  // df is per-document
  const idf = Math.log((totalPassages - df + 0.5) / (df + 0.5) + 1);  // BUT totalPassages is GLOBAL!
  
  statements.push(
    this.db.prepare(
      'INSERT OR REPLACE INTO bm25_idf (term, document_id, passage_id, term_frequency) VALUES (?, ?, ?, ?)'
    ).bind(term, df, idf, Date.now())
  );
}
```

**Schema Issue:**

```sql
-- ilka/migrations/0001_initial_schema.sql:45
CREATE TABLE IF NOT EXISTS bm25_idf (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  term TEXT NOT NULL UNIQUE,  -- ‚ùå KEYED ONLY BY TERM!
  document_frequency INTEGER NOT NULL,
  idf_score REAL NOT NULL,
  updated_at INTEGER NOT NULL DEFAULT (unixepoch())
);
```

**Why This Is Broken:**

1. **Mixed Corpus Size:** `totalPassages` is global (counts ALL passages across ALL users/documents)
2. **Per-Document DF:** `df = docSet.size` is per-document (only counts passages in current document)
3. **Global Overwriting:** `bm25_idf` is keyed only by `term`, so each new document overwrites the global IDF
4. **Result:** IDF scores become meaningless - they mix global corpus counts with per-document frequencies

**Example Scenario:**
- User A uploads Doc1 with 100 passages, term "education" appears in 50 ‚Üí IDF = log((100-50+0.5)/(50+0.5)+1) = 0.001
- User B uploads Doc2 with 10 passages, term "education" appears in 2 ‚Üí IDF = log((110-2+0.5)/(2+0.5)+1) = 4.08
- **BUG:** User A's IDF for "education" is now 4.08 (completely wrong!)

**RAG Does It Right:**

```typescript
// rag/worker/src/agr/bm25.ts:62
const totalPassages = passages.length;  // ‚úÖ Scoped to THIS document

// Schema is scoped per method:
// INSERT OR REPLACE INTO bm25_idf (term, method_id, document_frequency, idf_score)
// ‚úÖ method_id keeps each method's IDF separate
```

**Impact:** üî¥ CRITICAL - BM25 scores are completely unreliable. Top hits are random.

---

## ISSUE 2: RETRIEVAL LOOP STOPS TOO EARLY ‚úÖ CONFIRMED

### The Problem

**Location:** `ilka/src/methods/method2-bm25-agents.ts:37` and `ilka/src/agents/supervisor.ts:12`

**Confirmed Code:**

```typescript
// ilka/src/methods/method2-bm25-agents.ts:26-49
let reranked = await this.reranker.embeddingRerank(query, bm25Results.slice(0, 50), 0.7);
let topPassages = reranked.slice(0, 20);

while (round < 2) {
  const decision = await this.supervisor.decide(query, topPassages, round);
  // ‚ùå Supervisor ONLY looks at passages, NOT the generated answer!

  if (decision.action === 'proceed') break;

  if (decision.action === 'widen' && decision.strategy) {
    topPassages = await this.contextMaker.widen(topPassages, decision.strategy);
    round++;
  } else {
    break;
  }
}

const answer = await this.writer.write(query, topPassages);
// ‚ùå Answer is generated AFTER widening decisions are made!
```

**Supervisor Logic:**

```typescript
// ilka/src/agents/supervisor.ts:12-22
async decide(query: string, passages: ScoredPassage[], round: number): Promise<AgentDecision> {
  // ‚ùå No answer parameter!
  
  if (passages.length >= 10 && passages[0].score > 5.0) {
    return { action: 'proceed', reasoning: 'Strong signal with sufficient passages' };
    // ‚ùå This threshold is meaningless because BM25 scores are broken (Issue #1)
  }

  if (round >= 2) {
    return { action: 'proceed', reasoning: 'Maximum widening rounds reached' };
  }

  if (passages.length < 5 || passages[0].score < 3.0) {
    return { action: 'widen', strategy: 'heading-bounded', ... };
  }
  
  // ‚ùå Never checks if the ANSWER is actually good!
}
```

**Why This Is Broken:**

1. Supervisor decides based on **passage scores**, not **answer quality**
2. Because BM25 scores are inflated (Issue #1), `passages[0].score > 5.0` almost always passes
3. No feedback loop - can't react to poor answers
4. Widening happens **before** seeing if the answer is bad

**RAG Does It Right:**

```typescript
// rag/worker/src/agr/engine.ts:84-120 (approximate line numbers)
let answer = await this.writer.writeAnswer(query, blocks);
let verification = this.verifier.verify(answer, blocks);

while (round < 3 && !verification.passed) {
  // ‚úÖ Check if answer is bad FIRST
  
  const shouldWiden = await this.supervisor.shouldWiden({
    query,
    passages,
    blocks,
    answer,  // ‚úÖ Supervisor sees the answer!
    round
  });

  if (!shouldWiden) break;

  const strategy = await this.supervisor.selectWideningStrategy(state);
  blocks = await this.contextMaker.expandPassages(passages, strategy);
  
  // ‚úÖ Regenerate answer with wider context
  answer = await this.writer.writeAnswer(query, blocks);
  verification = this.verifier.verify(answer, blocks);
  round++;
}
```

**Impact:** üî¥ CRITICAL - Widening rarely happens when it should; bad answers are never retried.

---

## ISSUE 3: CONTEXT WIDENING DUMPS ENTIRE DOCUMENTS ‚úÖ CONFIRMED

### The Problem

**Location:** `ilka/src/agents/context-maker.ts:57`

**Confirmed Code:**

```typescript
// ilka/src/agents/context-maker.ts:85-119
private async expandToFullSection(passages: ScoredPassage[]): Promise<ScoredPassage[]> {
  const expanded: ScoredPassage[] = [];
  const processedSections = new Set<string>();

  for (const passage of passages) {
    const sectionKey = passage.document_id;  // ‚ùå Uses document_id as section key!
    
    if (processedSections.has(sectionKey)) continue;
    processedSections.add(sectionKey);

    // ‚ùå Get ALL passages from the ENTIRE document
    const allPassages = await this.getAllDocumentPassages(passage.document_id);

    if (allPassages.length === 0) {
      expanded.push(passage);
      continue;
    }

    const mergedText = allPassages.map(p => p.text).join('\n\n');
    // ‚ùå This can be 10+ paragraphs of mostly irrelevant text!
    
    expanded.push({
      ...passage,
      text: mergedText,  // ‚ùå ENTIRE DOCUMENT dumped into one "passage"
      start_line: Math.min(...allPassages.map(p => p.start_line)),
      end_line: Math.max(...allPassages.map(p => p.end_line)),
      ...
    });
  }

  return expanded.slice(0, 20);
}
```

**Why This Is Broken:**

1. `expandToFullSection` uses `document_id` as the section key, not actual sections
2. `getAllDocumentPassages()` returns **every passage** in the document
3. All passages are concatenated into a single massive text blob
4. Writer gets overwhelmed with irrelevant text
5. LLM struggles to find signal in noise

**RAG Does It Right:**

```typescript
// rag/worker/src/agr/agents/context-maker.ts:22-60
private async expandByHeading(passage: ScoredPassage): Promise<ExpandedBlock | null> {
  if (!passage.heading) return null;

  const query = `
    SELECT * FROM passages
    WHERE method_id = ?
      AND file_id = ?
      AND heading = ?  -- ‚úÖ Only passages with SAME heading
    ORDER BY start_line ASC
  `;

  const result = await this.db.prepare(query)
    .bind(this.methodId, passage.fileId, passage.heading)
    .all();

  if (!result.results || result.results.length <= 1) {
    return null;  // ‚úÖ Don't expand if section is tiny
  }

  const sectionPassages = result.results as any[];
  const combinedText = sectionPassages.map((p: any) => p.text).join("\n\n");
  // ‚úÖ Only combines passages from SAME SECTION
  
  return {
    text: combinedText,
    fileName: passage.fileName,
    startLine: Math.min(...sectionPassages.map((p: any) => p.start_line)),
    endLine: Math.max(...sectionPassages.map((p: any) => p.end_line)),
    heading: passage.heading,
    expandedBy: "heading-bounded"  // ‚úÖ Tracks expansion strategy
  };
}
```

**Impact:** üî¥ CRITICAL - Writer drowns in noise, can't find relevant info.

---

## ISSUE 4: ANSWER GENERATION IS WEAKER ‚úÖ CONFIRMED

### The Problem

**Location:** `ilka/src/agents/writer.ts:9`

**Confirmed Code:**

```typescript
// ilka/src/agents/writer.ts:9-35
async write(query: string, passages: ScoredPassage[]): Promise<Answer> {
  const startTime = Date.now();
  
  const evidenceBlocks = passages.slice(0, 20).map((p, idx) => {
    return `[${idx + 1}] ${p.heading || 'Passage'} [${p.start_line}-${p.end_line}]\n${p.text}`;
    // ‚ùå Generic "Passage" label, no file name
  }).join('\n\n---\n\n');

  const prompt = `Answer this question using ONLY the provided evidence blocks.
  // ‚ùå No system message - this is a USER prompt!

Question: ${query}

Evidence:
${evidenceBlocks}

Rules:
1. Every sentence must cite a source using [block-number]
2. If information is not in the blocks, state "I don't know"
3. Be concise and direct
4. Synthesize information across blocks when relevant

Answer:`;

  const response = await this.env.AI.run('@cf/openai/gpt-oss-120b', {
    input: [{ role: 'user', content: prompt }],  // ‚ùå No system message!
    max_output_tokens: 1500,
    temperature: 0.15
  });

  // ...
  
  return {
    method: 'writer',
    text: answerText,
    citations,
    confidence: citations.length > 0 ? 'high' : 'low',
    latency_ms: Date.now() - startTime
  };
}

private extractCitations(text: string, passages: ScoredPassage[]) {
  // ...
  return Array.from(citedIndices)
    .map(idx => {
      const passage = passages[idx];
      return {
        file_name: 'document',  // ‚ùå Hard-coded generic label!
        start_line: passage.start_line,
        end_line: passage.end_line,
        text: passage.text.substring(0, 200)
      };
    });
}
```

**Why This Is Broken:**

1. **No System Prompt:** Grounding instructions are in user message, less effective
2. **Generic Citations:** `file_name: 'document'` - user can't tell which file
3. **Weak Citation Format:** `[block-number]` instead of `[filename:lines]`
4. **No Structural Guidance:** Doesn't enforce proper citation format in output

**RAG Does It Right:**

```typescript
// rag/worker/src/agr/agents/writer.ts:8-45
constructor(
  private readonly openai: OpenAIClient,
  private readonly systemPrompt: string  // ‚úÖ System prompt passed in
) {}

async writeAnswer(query: string, blocks: ExpandedBlock[]): Promise<string> {
  const contextBlock = blocks
    .map((block, index) => {
      const citation = `[${block.fileName}:${block.startLine}-${block.endLine}]`;
      // ‚úÖ Real filename, real line ranges
      const heading = block.heading ? `\nHeading: ${block.heading}` : "";
      return `Source ${index + 1} ${citation}${heading}\n${block.text}`;
    })
    .join("\n\n---\n\n");

  const prompt = `Answer the following question using ONLY the provided sources. 

Every statement must cite a source using the format [filename:startLine-endLine].
// ‚úÖ Explicit citation format

If the information is not in the sources, say "I do not have enough information to answer this."

Sources:
${contextBlock}

Question: ${query}

Answer with citations:`;

  const response = await this.openai.respond("gpt-5-mini", [
    { role: "system", content: this.systemPrompt, ... },  // ‚úÖ System prompt!
    { role: "user", content: prompt, ... }
  ]);

  return response.trim();
}
```

**Impact:** üü° MODERATE - Answers are less grounded, citations are less useful.

---

## ISSUE 5: QUERY ANALYSIS SIGNALS NOT USED ‚úÖ CONFIRMED

### The Problem

**Location:** `ilka/src/methods/method1-bm25-direct.ts:33`

**Confirmed Code:**

```typescript
// ilka/src/query/analyzer.ts generates:
{
  "synonyms": ["...", "..."],
  "related_terms": ["...", "..."],
  "rephrasings": ["...", "...", "..."],
  "hypothetical_answer": "...",
  "recommended_methods": ["...", "..."],
  ...
}

// But method1-bm25-direct.ts:23-33:
async execute(query: string, analysis: QueryAnalysis, userId: string): Promise<Answer> {
  // ‚ùå analysis parameter is received but never used!
  
  const normalizedQuery = this.bm25.preprocessQuery(query);
  // ‚ùå Uses original query, ignores synonyms
  
  let bm25Results = await this.bm25.search(normalizedQuery, { topK: 100, userId });
  // ‚ùå No synonym expansion, no HYDE, no rephrasing

  if (bm25Results.length === 0) {
    const fuzzyResult = await this.bm25.fuzzySearch(query, { userId });
    // ‚ùå Falls back to typo matching, not semantic expansion
    if (fuzzyResult) {
      bm25Results = fuzzyResult.results;
    }
  }
  
  // ... rest ignores analysis entirely
}
```

**Why This Is Broken:**

1. Query analyzer generates rich signals: synonyms, related terms, HYDE
2. **None of it is used**
3. BM25 search uses literal query tokens only
4. Misses semantically related passages
5. No query expansion, no rephrasing

**RAG's Deep Search Uses It:**

```typescript
// rag/worker/src/agr/reasoning.ts:71 (approximate)
async generateSearchPlan(query: string): Promise<SearchPlan> {
  const prompt = `Given this question, generate:
1. Core search terms (2-4 keywords)
2. Synonyms and related concepts
3. Broader context terms
...`;

  const response = await this.openai.respond(...);
  const plan = JSON.parse(response);
  
  // ‚úÖ Actually USES the expanded terms
  const results = await this.bm25.search(plan.expandedTerms.join(' '), ...);
  
  return {
    originalQuery: query,
    expandedTerms: plan.coreTerms.concat(plan.synonyms, plan.contextTerms),
    passages: results
  };
}
```

**Impact:** üü° MODERATE - Misses semantically related content, requires exact keyword matches.

---

## ISSUE 6: VERIFICATION ONLY CHECKS SURFACE HEURISTICS ‚úÖ CONFIRMED

### The Problem

**Location:** `ilka/src/agents/verifier.ts:6`

**Confirmed Code:**

```typescript
// ilka/src/agents/verifier.ts:6-34
async verify(answer: Answer): Promise<{ passed: boolean; issues: string[] }> {
  const issues: string[] = [];

  if (answer.citations.length === 0) {
    issues.push('No citations found');
    // ‚úÖ This check is OK
  }

  if (answer.text.length < 50) {
    issues.push('Answer too short');
    // ‚úÖ This check is OK
  }

  const sentences = answer.text.split(/[.!?]+/).filter(s => s.trim().length > 0);
  const citedSentences = sentences.filter(s => /\[\d+\]/.test(s)).length;
  const citationRate = citedSentences / sentences.length;

  if (citationRate < 0.5) {
    issues.push(`Low citation rate: ${Math.round(citationRate * 100)}%`);
    // ‚úÖ This check is OK
  }

  if (answer.text.toLowerCase().includes("i don't know") && answer.text.length < 100) {
    issues.push('Answer indicates lack of information');
    // ‚úÖ This check is OK
  }

  // ‚ùå BUT: Never validates if citations actually match passages!
  // ‚ùå Never checks if cited text supports the claim!
  // ‚ùå Just counts [n] markers, doesn't verify they're valid!

  const passed = issues.length === 0;
  return { passed, issues };
}
```

**Why This Is Broken:**

1. Only checks **citation count**, not **citation validity**
2. Doesn't verify citations reference actual passages
3. Doesn't check if cited text supports claims
4. Could pass an answer with `[999]` citations that don't exist
5. No grounding verification

**RAG Does It Right:**

```typescript
// rag/worker/src/agr/agents/verifier.ts:23-60
verify(answer: string, blocks: ExpandedBlock[]): VerificationResult {
  const issues: string[] = [];
  
  // ‚úÖ Same basic checks as ilka
  if (answer.toLowerCase().includes("i do not")) { ... }
  
  const citationPattern = /\[([^\]]+):(\d+)-(\d+)\]/g;
  const citations = Array.from(answer.matchAll(citationPattern));
  
  // ‚úÖ Then VALIDATES each citation:
  for (const citation of citations) {
    const [, fileName, startLine, endLine] = citation;
    const matchingBlock = blocks.find(
      b => b.fileName === fileName && 
           b.startLine <= parseInt(startLine) && 
           b.endLine >= parseInt(endLine)
    );

    if (!matchingBlock) {
      issues.push(`Citation ${citation[0]} does not match any source block`);
      // ‚úÖ Catches hallucinated citations!
    }
  }
  
  if (uncitedSentences > sentences.length * 0.3) {
    issues.push(`${uncitedSentences} sentences lack citations`);
    // ‚úÖ More strict than ilka's 50% threshold
  }

  const passed = issues.length === 0 && citationCount > 0;
  return { passed, issues, citationCount, uncitedSentences };
}
```

**Impact:** üü° MODERATE - Bad answers with fake citations can pass verification.

---

## ARCHITECTURAL COMPARISON

### ILKA/ZYN Flow (Broken)

```
Query ‚Üí Analyzer (generates rich signals)
         ‚Üì (signals ignored)
       BM25 Search (literal query only)
         ‚Üì
       Rerank
         ‚Üì
       Supervisor (checks passage scores, not answer)
         ‚Üì (rarely widens because scores inflated)
       Context Maker (dumps entire document)
         ‚Üì
       Writer (no system prompt, generic citations)
         ‚Üì
       Verifier (surface checks only)
         ‚Üì
       Return answer (no retry even if bad)
```

### RAG Flow (Correct)

```
Query ‚Üí Deep Search Planner (generates search terms)
         ‚Üì (actually uses expanded terms)
       BM25 Search (with synonyms + related terms)
         ‚Üì
       Rerank
         ‚Üì
       Writer (generates answer with system prompt)
         ‚Üì
       Verifier (validates citations against blocks)
         ‚Üì
       Supervisor (checks answer quality + verification)
         ‚Üì (if bad, widens)
       Context Maker (stitches just heading/adjacent)
         ‚Üì (retry loop)
       Writer (regenerate with wider context)
         ‚Üì
       Verifier (recheck)
         ‚Üì
       Return answer (only after verification passes)
```

---

## IMPACT SUMMARY

| Issue | Severity | Impact |
|-------|----------|--------|
| 1. BM25 IDF Overwriting | üî¥ CRITICAL | Scores completely unreliable, top hits are random |
| 2. No Answer-Based Widening | üî¥ CRITICAL | Bad answers never retried, widening rarely happens |
| 3. Context Dumps Entire Doc | üî¥ CRITICAL | Writer drowns in irrelevant text |
| 4. Weak Answer Generation | üü° MODERATE | Less grounded answers, generic citations |
| 5. Analyzer Signals Ignored | üü° MODERATE | Misses semantic matches |
| 6. Surface-Only Verification | üü° MODERATE | Fake citations pass |

**Overall Assessment:** üî¥ **CRITICAL SYSTEM FAILURE**

The combination of broken BM25 scoring + no answer feedback + context dumping creates a perfect storm. Even if one issue were fixed, the others would still cripple the system.

---

## RECOMMENDED FIXES (In Order of Priority)

### 1. Fix BM25 IDF Scoping (CRITICAL)

```sql
-- Change schema to scope IDF per user or per document:
ALTER TABLE bm25_idf ADD COLUMN user_id TEXT;
ALTER TABLE bm25_idf ADD COLUMN document_id TEXT;
DROP INDEX idx_bm25_idf_term;
CREATE UNIQUE INDEX idx_bm25_idf_term_doc ON bm25_idf(term, document_id);

-- Or adopt RAG's approach: scope per method_id
```

```typescript
// In bm25.ts:57, use per-document corpus size:
const totalPassages = passages.length;  // Not global!

// Update IDF calculation to be per-document:
const idf = Math.log((totalPassages - df + 0.5) / (df + 0.5) + 1);
```

### 2. Implement Answer-Based Feedback Loop (CRITICAL)

```typescript
// In method2-bm25-agents.ts, move answer generation inside loop:
while (round < 3) {
  const answer = await this.writer.write(query, topPassages);
  const verification = await this.verifier.verify(answer, topPassages);
  
  if (verification.passed) break;  // Only stop if answer is good!
  
  const decision = await this.supervisor.decide(query, topPassages, answer, round);
  if (decision.action === 'widen') {
    topPassages = await this.contextMaker.widen(topPassages, decision.strategy);
    round++;
  } else {
    break;
  }
}
```

### 3. Fix Context Maker to Use Actual Sections (CRITICAL)

```typescript
// In context-maker.ts:85, change expandToFullSection:
private async expandToFullSection(passages: ScoredPassage[]): Promise<ScoredPassage[]> {
  const expanded: ScoredPassage[] = [];
  
  for (const passage of passages) {
    // Use actual section, not entire document!
    const sectionKey = passage.parent_section_id || passage.heading;
    
    if (!sectionKey) {
      expanded.push(passage);  // No section, use as-is
      continue;
    }
    
    const sectionPassages = await this.getSectionPassages(
      passage.document_id,
      sectionKey
    );
    
    // Limit to reasonable size (e.g., max 5 passages)
    const limitedPassages = sectionPassages.slice(0, 5);
    const mergedText = limitedPassages.map(p => p.text).join('\n\n');
    
    expanded.push({ ...passage, text: mergedText, ... });
  }
  
  return expanded;
}
```

### 4. Add System Prompt to Writer (MODERATE)

```typescript
// In writer.ts:9, add system message:
const response = await this.env.AI.run('@cf/openai/gpt-oss-120b', {
  input: [
    { 
      role: 'system', 
      content: 'You are a grounded QA system. Every statement must cite sources using [filename:startLine-endLine] format. Never make claims without citations.'
    },
    { role: 'user', content: prompt }
  ],
  max_output_tokens: 1500,
  temperature: 0.15
});

// Fix citation extraction to use real filenames:
private extractCitations(text: string, passages: ScoredPassage[]) {
  // ... existing code ...
  return Array.from(citedIndices).map(idx => {
    const passage = passages[idx];
    return {
      file_name: passage.file_name || 'unknown',  // Use real filename!
      start_line: passage.start_line,
      end_line: passage.end_line,
      text: passage.text.substring(0, 200)
    };
  });
}
```

### 5. Use Query Analyzer Signals (MODERATE)

```typescript
// In method1-bm25-direct.ts:33, use analysis:
async execute(query: string, analysis: QueryAnalysis, userId: string): Promise<Answer> {
  // Expand query with synonyms:
  const expandedTerms = [
    ...this.bm25.tokenize(query),
    ...analysis.synonyms.flatMap(s => this.bm25.tokenize(s)),
    ...analysis.related_terms.flatMap(t => this.bm25.tokenize(t))
  ];
  
  const expandedQuery = expandedTerms.join(' ');
  let bm25Results = await this.bm25.search(expandedQuery, { topK: 100, userId });
  
  // ... rest of method
}
```

### 6. Validate Citations in Verifier (MODERATE)

```typescript
// In verifier.ts:6, add citation validation:
async verify(answer: Answer, passages: ScoredPassage[]): Promise<{ passed: boolean; issues: string[] }> {
  const issues: string[] = [];
  
  // ... existing checks ...
  
  // NEW: Validate each citation
  const citationPattern = /\[(\d+)\]/g;
  const citations = Array.from(answer.text.matchAll(citationPattern));
  
  for (const match of citations) {
    const idx = parseInt(match[1]) - 1;
    if (idx < 0 || idx >= passages.length) {
      issues.push(`Citation [${match[1]}] is out of range`);
    }
  }
  
  const passed = issues.length === 0;
  return { passed, issues };
}
```

---

## CONCLUSION

**The smart person's analysis is 100% accurate.** Every issue they identified exists in the ilka/zyn codebase and collectively explains why the system struggles.

**Root Cause:** The system was designed with the right architecture (multi-agent, widening, verification) but critical implementation bugs prevent it from working as intended.

**Path Forward:** Fix issues 1-3 first (BM25 scoping, feedback loop, context maker). These are showstoppers. Issues 4-6 are enhancements that improve quality but don't break core functionality.

**Estimated Effort:**
- Issue 1: 4-6 hours (schema migration + code changes)
- Issue 2: 2-3 hours (refactor method2)
- Issue 3: 1-2 hours (fix context-maker logic)
- Issues 4-6: 2-3 hours each

**Total:** ~15-20 hours to fix all critical issues.

---

**Report prepared by:** Factory Droid  
**Date:** October 17, 2025  
**Status:** ‚úÖ All issues confirmed, fixes documented
